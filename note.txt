org.apache.hadoop.hbase.PerformanceEvaluation

By default, maven stores its artifacts in
/home/shen/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0

IMPORTANT:
Moore curve helps to reduce the amount of scans. However, since hfile blocks 
does not respect Moore curve block (quad-tree block) boundaries. we may end
up fetching/scan a considerable amount of unnecessary hfile blocks. This is
less a issue in 1D applications, as they only fetch 2 more hfile block, which
is small enough when compared to a considerable scan range (n). However, in
2D (3D) applications, a geoScan may end up in many small scans with each
fetching (n+2)^2 - n^2 more hfile blocks.

  solution: 
  allow hfile blocks to respect moore cuve blocks. When flushing memstore into
  disk, it determines whether a quad-tree square needs to be split into 4 
  smaller ones, wich each form an separate hfile block.
  aloow scan to fetch multiple consequentive hfile blocks.


TODO:
2. continue to learn HFileBlockIndex, think about how we should generate the
   index for the moore files.
3. HFileBlockIndex.BlockIndexReader holds secondary indice to achieve finer 
   granularity? Find out the detailed format.
   A: only block-level indics. The index is kept in a hierarchical structure.
4. Look at StoreFileScanner to learn how the scanner read data from HFiles.

Question:
1. can a client call one region server to get data from another one?
  A: in the current implementation, NO. I current understanding is that, the
     RSRpcServer still need to extract the region information from GetRequest
     as RSRpcServer and HRegion do not form a 1-to-1 binding. One physical
     server may host multiple HRegions, but only one RSRpcServer.
2. which object caches region server info on the client side?
  A: HConnection, whose implementation is 
     ConnectionManager.HConnectionImplementation.
3. how does the server side achieves concurrency?
  A: In the RpcServer.java, the listener has a fixed number of Readers in an
     ExecutorPool, 10 by default.
4. which component triggers MemStore flush and HFile compaction?

Remember:
1. getStub() returns a ClientService.BlockingInterface object
2. APPEND, INCREMENT, PUT, DELETE are all encapsulated by a mutate request. 
   But it seems that the encapsulation for the PUT request is never used.
   Instead, every PUT request is handled using the multi() API exposed by
   the RSRpcServices. Remaining actions are handled using mutate.
3. The RPC part should be reusable. 
4. The wrapper (Storefile) of HFile should be reusable.
5. It seems that we should be able to reuse a large part of HFile 
   implementation, as the moore serialization will finally serialize it into
   a 1-D key range. But, we need to reimplement how the index is generated,
   and how the file is queried.
6. A store has a single StoreScanner, but a StoreScanner may contain multiple
   StoreFileScanner and MemStoreScanners(in DefaultMemStore). They have a
   common ancestor: KeyValueScanner. KeyValueHeap also implements 
   KeyValueScanner, as both StoreScanner and RegionScanner uses KeyValueHeap
   to aggregate data from multiple sources.

Call Traces:
0. init RPC info:
  RPC CLIENT SIDE:
  client/HTable.java      333     initialize the configuration based on
                                  ClusterConeciton.getConfiguration().
                                  This configuration was later passed to
                                  RpcControllerFactory.instantiate(...) to
                                  create a new rpcControllerFactory. Then the
                                  RpcControllerFactory object is passed to the
                                  AsyncProcess constructor to grant it 
                                  knowledge about the RPC info.
  ipc/rpcControllerFactory.java
                          56      based on the information in the configuration
                                  it creates a new RpcControllerFactory using
                                  ReflectionUtils.instantiateWithCustomCtor.
1. get request
  RPC CLIENT SIDE:
  client/HTable.java      801     get() declares a subclass of abstract class
                                  RegionServerCallable at line 728. In that 
                                  definition, it constructs the request object 
                                  by calling 
                                  RequestConverter.buildGetRequest(), then the
                                  get request is issued using 
                                  ClientService.BlockingInterface.get().
                                    In order to get the right region servers
                                  to query with, RegionServerCallable has a 
                                  field location, which can be returned by 
                                  calling getLocation(). The location field
                                  is initialized by 
                                  HConnection.getRegionLocation() whose impl is
                                  ConnectionManager.HConnectionImplementation.
  client/ConnectionManager.java
                          464     class HConnectionImplementation locates the
                                  correct set of region servers by calling
                                  locateRegion(...) on line 982. If the info
                                  is in cache, Registry.getMetaRegionLocation()
                                  will return it. Otherwise, it calls 
                                  locateRegionInMeta() on line 995.
  client/ConnectionManager.java
                          1003    locateRegionInMeta() first checks if it
                                  should look into the cache on line 1008.
                                  getCachedLocation(table, row) returns the in
                                  cache data. MetaScanner.getHRegionInfor()
                                  on line 1060 converts the row result to
                                  HRegionLocation.

  protobuf/RequestConverter.java
                          157     buildGetReuest() calls 
                                  buildRegionSpecifier() on line 159 to build
                                  a RegionSpecifier. Then it uses 
                                  GetRequest.Builder to build the GetRequest.
                                  The GetRequest contains the encoded name of
                                  the target region, which is passed through
                                  the RPC call.


  RPC SERVER SIDE:
  hbase-server/regionserver/...
  RSRpcServices.java      1562    get() first identify the right region to
                                  query by translating the encoded name of 
                                  the region into an HRegion object.
                                  Then it calls HRegion.getClosestRowBefore()
                                  and HRegion.get() on line 1583 and 1590
                                  respectively.
  HRegion.java            4735    Result get() wraps List<Cell> get()
                                  method on line 4747.
  HRegion.java            4756    List<Cell> get() calls RegionScanner.next()
                                  on line 4773.
  HRegion.java            3698    RegionScannerImpl implements the interface
                                  RegionScanner.
  HRegion.java            3817    next() calls nextRaw() on line 3826, which
                                  then calls nextInternal() on line 3847.
  HRegion.java            3918    nextInternal() calls KeyValueHeap.peek() to 
                                  get the current, it also calls
                                  RgionScannerImpl.nextRow() on line 3970 and
                                  4001 to get next row.
  HRegion.java            4072    nextRow() calls KeyValueHeap.next()
  KeyValueHeap.java       134     updates the heap. HRegion calls (line 3764) 
                                  HStore.getScanner() on all stores to passe
                                  StoreScanner or ReversedStoreScanner to
                                  KeyValueHeap during construction. 
  HStore.java             1778    getScanner()
  StoreScanner.java       445     next() calls current.next()
  StoreFileScanner.java   139   
  HFileScanner.java   
  AbstractHFileReader.java        see 7 seek key.

2. scan request
  RPC SERVER SIDE:
  RSRpcService.java       1798    scan(). 

  How scan seeks to the first:
  RSRpcServices.java      1872    region.getScanner()
  HRegion.RegionScannerImpl.java            
                          3764    store.getScanner()
  HStore.java             1778    getScanner()
  StoreScanner.java       185     seekScanners()


3. put request
  Surprisingly, the put request is done IN BATCH on the client side.
  RPC CLIENT SIDE:
  hbase-client/...
  client/HTable.java      965     put() calls doPut().
  client/HTable.java      982     doPut() first add the put request into 
                                  its writeAsyncBuffer. When the buffer size
                                  is larger than a threshold, it calls
                                  backgroundFlushCommits() to the server side.
  client/HTable.java      1009    backgroundFlushCommits() calls 
                                  AsyncProcess.submit() to push the changes.
  client/AsyncProcess.java
                          277     submit(...) calls createAsyncRequestFuture()
                                  which in turn constructs an 
                                  AsyncRequestFutureImpl object, then it calls
                                  AsyncRequestFutureImpl.sendMultiAction() to
                                  flush the actions.
  client/AsyncProcess.java
                          513     class AsyncRequestFutureImpl
  client/AsyncProcess.java
                          586     AsyncRequestFutureImpl.sendMultiAction()
                                  calls RpcRetryingCaller.callWithoutRetries() 
                                  on line 602 to send actions and wait for
                                  results. The callable implementation is 
                                  MultiServerCallable
  client/MultiServerCallable.java
                          84      call() finally gives controller
                                  and the requestProto to the 
                                  ClientService.blockingInterface.multi() to 
                                  get the response.

  RPC SERVER SIDE:
  hbase-server/.../regionserver/
  RSRpcServices.java      1626    multi() gets the RpcController and the
                                  MultiRequest objects through RPC. It then
                                  calls mutateRows(...) on line 1663.
  RSRpcServices.java      348     mutateRows(...) adds the put actions into
                                  row mutations object by using 
                                  ProtobufUtil.toPut. Then it calls
                                  HRegion.mutateRow().
  HRegion.java            4798    mutateRow(...) after a few thin warppers,
                                  it call reaches processRowsWithLocks on 
                                  line 4851.
  HRegion.java            4851    processRowsWithLocks() calls
                                  doProcessRowWithTimeout() on line 4905 to
                                  generate mutations into the variable 
                                  "mutations". Then, the result mutations are
                                  applied to the memstore on line 4916.
                                  MultiRowMutationProcessor.java is the
                                  RowProcessor implementation.
  HRegion.java            4979    doProcessRowWithTimeout() class 
                                  MultiRowMutationProcessor on line 5005. 

4. split a region
  RPC CLIENT SIDE:
  hbase-client/...
  client/HBaseAdmin.java  2007    split() calls the ProtobufUtil.split()
                                  method.
  protobuf/ProtobufUtil.java
                          1691    split() wraps the
                                  AdminService.BlockingInterface.splitRegion()
                                  RPC method.
                          

  RPC SERVER SIDE:
  hbase-server/regionserver/...
  RSRpcServices.java      683     The constructor exposes its own services
                                  to the rpcServer by calling getServices()
                                  on line 713.
  RSRpcServices.java      818     getServices() returns a list of all RPC
                                  services provided by RSRpcServices including 
                                  RSRpcServices.splitRegion() by 
                                  calling newReflectiveBlockingService(this),
                                  AdminService and ClientService are generated
                                  classes under
                                  hbase-protocol/.../protobuf/generaged.
                                  more about google's protobuf at:
                                  https://code.google.com/p/protobuf/
  RSRpcServices.java      1435    splitRegion() calls HRegion.checkSplit() 
                                  on line 1449 to get splitpoints. It then
                                  calls HRegion.startRegionOperation() to 
                                  acquire the lock. After that, it calls 
                                  HRegion.forceSplit() to set the split flag
                                  and splitting points. Finally, it calls
                                  CompactSplitThread.requestSplit() to force
                                  instant split.
  CompactSplitThread.java 214     requestSplit() uses a SplitRequest object as
                                  a parameter when calling
                                  ThreadPoolExecutor.execute() to execute the
                                  split request on line 221.
  SplitRequest.java       57      run() calls  SplitTransaction.execute() on
                                  line 82 to execute the split transaction.
  SplitTransaction.java   565     execute() calls createDaughters() which in
                                  turn calls stepsBeforePONR() to create two
                                  daughter regions. Then it calls
                                  stepsAfterPONR() which in turn calls 
                                  openDaughters() to open the two daughter 
                                  regions.
  HRegion.java            5658    checkSplit() calls 
                                  RegionSplitPolicy.getSplitPoint() on line 
                                  5678 to get split points
  RegionSplitPolicy.java          as an abstract class has two implementations
                                  IncreasingToUpperBoundRegionSplitPolicy.java
                                  and ConstantSizeRegionSplitPolicy.java

  SplitTransaction.java            578     stepsAfterPONR() calls openDaughters() to
                                  perform the actual open daughter opertaion.
                                  Which then calls HRegion.openHRegion();

  Solution: implement in SplitTransaction line 441. The transitionZKNode() method
  blocks to wait for master to react. So, I can add a field to let master node that
  we would like to reuse files. 

  So the steps are: 1. client send the request to region server
                    2. region splits it self, and notify master
                    3. master move new regions

5. flush MemStore to HFiles
  hbase-server/.../regionserver
  HRegion.java            4851    In processRowsWithLocks(), it checks whether
                                  the memstore is getting too large by calling
                                  isFlushSize() on line 4969. In fact, 
                                  isFlushSize() is called every time some 
                                  mutation is performs: 
                                    append() on line 5042, called at line 5191
                                    increment() on 5232, called at 5384
                                    batchMutate() on 2254, called at 2280
                                    processRowsWithLocks()
                                  Whenever isFlushSize() returns True, the
                                  requestFlush() method will be called.
  HRegion.java            3037    requestFlush() get a FlushRequester from
                                  RegionServerServices.getFlushRequester().
                                  In the current implementation, HRegionServer
                                  implements RegionServerServices, and the
                                  impl of FlushRequester is MemStoreFlusher,
                                  which implements the requestFlush() method.
  MemStoreFlusher.java    340     requestFlush() returns near immediately, it
                                  only put a FlushRegionEntry object into the 
                                  flushQueue. MemStoreFlusher implements a 
                                  pool of FlushHandler those do flushes in the 
                                  backend. Each flush will call the 
                                  flushRegion() method.
  MemStoreFlusher.java    417     flushRegion() calls nofityFlushRequst() on
                                  line 477, and calls HRegion.flushcache() on
                                  line 478.
  MemStoreFlusher.java    510     notifyFlushRequest() notify all
                                  FlushRequestListener objects in 
                                  flushRequestListeners. An HRegionServer owns
                                  the MemStoreFlusher, and it passes the 
                                  MemStoreFlusher when creating the
                                  HeapMemoryManager object. The 
                                  HeapMemoryManager registers a 
                                  HeapMemoryTunerChore object as a 
                                  FlushRequestListener when the manager starts.
  HRegion.java            1547    flushcache() calls internalFlushcache() on
                                  line 1590.
  HRegion.java            1669    internalFlushcache() calls 
                                  StoreFlushContext.flushCache() on line 1787.
                                  The implementation is StoreFlusherImpl on
                                  line 1950 of HStore.java. The storeFlushCtxs
                                  object of HRegion contains StoreFlushContexts
                                  of all stores in this region. 
  HStore.java             1970    StoreFlusherImpl.flushCache() calls
                                  HStore.flushCache() on line 1970.
  HStore.java             787     flushCache() StoreFlusher.flushSnapshop() on
                                  line 798. StoreFlusher has two 
                                  implementations DefaultStoreFlusher and 
                                  StripeStoreFlusher. StoreFlusher is a member
                                  of StoreEngine. HStore passes itself to 
                                  StoreEngine on creation.
  DefaultStoreFlusher.java
                          44      flushSnapshot() constructs an InternalScanner
                                  from the MemStoreSnapshot object on line 52. 
                                  It also constructs a StoreFile.Writer for its
                                  HStore by calling HStore,createWriterInTmp()
                                  on line 64.
                                  Then, it calls StoreFlusher.performFlush().
                                  The HStore object is passed to it during its
                                  construction.
  StoreFlusher.java       110     performFlush() writes data from the scanner
                                  into the sink. Data is ordered during 
                                  flushing. StoreFlusher itself is a abstract
                                  class. Its subclasses are DefaultStoreFlusher
                                  and StripStoreFlusher.

6. data format
  HRegion.java            254     variable stores maps column family names into
                                  HStore object. On line 795, inside the
                                  initializeRegionStores method, "stores" are
                                  add to variable stores one by one. The add()
                                  method initially put the kv into the memstore
                                  of the called store.
  hbase-server/.../io/hfil/...
  HFile.java              291     defines the interface Writer
  AbstractHFileWriter.java
                          48      implements HFile.Writer
  HFileWriterVx                   extends AbstractHFileWriter

7. seek key
  HFileReaderV2.java      558     AbstractScannerV2 extends 
                                  AbstractHFileReader.Scanner which in turn 
                                  implements HFileScanner.
  HFileReaderV2.java      645     AbstractScannerV2.seekTo(Cell, boolean) calls
                                  loadBlockAndSeekToKey() on line 653.
  HFileReaderV2.java      750     ScannerV2 extends AbstractScannerV2 which
                                  defines loadBlockAndSeekToKey() on line 897.
  HFileReaderV2.java      897     loadBlockAndSeekToKey().
  HFileReaderV2.java      987     blockSeek()

8. HBase create HDFS output stream
  HFile.java              389     HFile.WriterFactory.create() calls
                                  createWriter who gets the outputstream from
                                  AbstractHFileWriter.createOutputStream(), 
                                  which in turn calls FSUtils.create(). Users
                                  may specify favoredNodes. FSUtils.create()
                                  calls DistributedFileSystem.create() on line
                                  292. which is a source file in HDFS. The
                                  call reaches the create() method on line 350
                                  of DistributedFileSystem.java in HDFS.
                                  It creates a DFSOutputStream that extends 
                                  FSOutputSummer, passes it to 
                                  HdfsDataOutputStream, and returns the
                                  HdfsDataOutputStream.

                                  HdfsDataOutputStream extends 
                                  FSDataOutputStream. DFSOutputStream is a 
                                  member variable of FSDataOutputStream.

                                  FSDataOutputStream takes the DFSOutputStream
                                  as a OutputStream, so it cannot seal a HDFS
                                  block. But, HdfsDataOutputStream can. So,
                                  I need to override the seal method in
                                  HdfsDataOutputStream to seal a block.
                                  Inside that method, it should call
                                  DFSOutputStream.seal(), which in turns
                                  calls FSOutputSummer.seal(); NOTE I, maybe I
                                  should not put it in FSOutputStream. NOTE II,
                                  do I also need to modify RawLocalFileSystem?

                                  DFSOutputStream.java explains how it works

                                  It seems using a non-full block to indicate
                                  the end of the file.

                                  add a field isSealed into LocatedBlock, which
                                  allows HDFS to distinguish intentionally 
                                  partially filled block from the last block

                                  but how to modify the datanode side.

                                  line 1321 NetUtils.getOutputStream() creates
                                  the blockStream. 

                                  check hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java
                                  In that file, check receiveBlock() and receivePacket()

                                  datanode/BlockSender.java sends blocks to client.

                                  It should be the receivePacket() method that we modify

                                  DataXceiver.java readBlock() calls BlockSender.sendBlock()
                                  DataXceiver.readBlock() is the server side method, while
                                  Sender.readBlock is the client side method

                                  Here is what happened: 
                                  1. client call Sender.readBlock(), the server will 
                                  receive an READ_BLOCK operation. Then the server
                                  calls DataXceiver.readBlock() to process.

                                  2. DataXceiver extends Receiver which is the
                                  protocol API. DataXceiver will be spawned as 
                                  a Daemon on DataXceiverServer. in DataXceiver.run(),
                                  it waits for operation from client side,
                                  and calls Receiver.processOp() to perocess
                                  the operation

DFSInputStream.java    430       getBlockAt() calls DFSClient.getLocatedBlocks(src, offet);
                                  which calls DFSClient.callGetBlockLocations() which calls
                                  namenode.getBlockLocations(), where the namenode is a 
                                  ClientProtocol instance. On namenode side, 
                                  the method getBlockLocations() is implementated by
                                  NameNodeRpcServer.java, which then calls FSNameSystem.getBlockLocations(),

                                  FSDirectory().getLastINodeInPath(), INode related code does
                                  not care about offset, instead, it returns inode information
                                  of that file. Then BlockManager.createLocatedBlocks() takes
                                  the offset and returns the right LocatedBlocks. It seems that
                                  BlockManager.createLocatedBlockList() allows viable length block.
                                  the process goes through all blocks and add together their length

DFSInputStream.java   1270        read(long position, ...) calls getBlockRange() 
                                  calls getFinallizedBlockRange(), which also calls
                                  dfsClient.getLocatedBlocks();

                                  So, the read path should be able to find the block
                                  even if we have viable blocks

                                  where is the shared packet structure defined? need to add an is sealed
                                  field: protocol.datatransfer.PacketHeader.java

DatanodeStorageInfo.java          Each Datanode has one or multiple DatanodeStorage. DatanodeStorageInfo
                                  has reference of its own Datanode on line 104 DatanodeDescriptor dn


How does hdfs determine locations to write blocks:
DFSOutputStream.java    1589      In constructor, set the favorite nodes. It can be reset by calling
                                  DFSOutputStream.DataStreamer.setFavoredNodes(). Then in 
                                  DataStreamer.locateFollowingBlock(), it passes the favoredNodes
                                  information to dfsClient.namenode.addBlock() method on
                                  line 1448, which reaches NameNodeRpcServer.addBlock() through
                                  the protocol. Which then calls FSNamesystem.getAdditionalBlock().
FSNamesystem.java       2702      getBlockManager().chooseTarget() to get DatanodeStorageInfo.
BlockManager.java       1436      chooseTarget() calls BlockPlacementPolicy.chooseTarget().

File Meta

FSNamesystem.java       1696      getBlockLocationsUpdateTimes() calls 
                                  FSDirectory.getLastINodeInPath() on line 1727

FSDirectory.java        2059      addINode();
BlockReceiver.java      732       
FSNamesystem.java       3901      getAdditionalBlocks() calls commitOrCompleteLastBlock();
                                  on 2750
                                  added a splitFileReuseBlocks() method.


9. move a region

on client side:
HBaseAdmin.java       1702        move(regionname, server) calls MasterRpcServices.moveRegion(),
                                  which then calls HMaster.move();
HMaster.java          1054        move(); which then calls AssignmentManager.balance(RegionPlan);
                                  which puts the RegionPlan object into this.regionPlans;

AssignmentManager.java            in balance() it calls unassign() to do the job
AssignmentManager.java            processRegionInTransitionAndBlockUntilAssigned() calls 
                                  processRegionsInTransition() calls handleRegionSplitting()
                                  which gets RegionPlan from regionPlans.

                                  handleAssignmentEvent() calls handleRegion() calls 
                                  handleRegionSplitting();

TODO: !!!!!! do not need to actually split the StoreFile into two real StoreFiles!! Reference should still be OK as the HDFS block will be hosted on the daughter server!!!!!
      1. continue StoreFlusher.performFlush()  
      4. create a configuration file for pre-splits: refer to hbase.apache.org/book/rowkey.design.html
